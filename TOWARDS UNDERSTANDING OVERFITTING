import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from scipy.optimize import fsolve
from scipy.interpolate import interp1d

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Numerically solve the PDE u''(x) + u(x)^2 - sin(pi*x) = 0 with u(0) = 0, u(1) = 0
def solve_pde_fd(n_points=100):
    x = np.linspace(0, 1, n_points)
    h = x[1] - x[0]
    def func(u_int):
        u_full = np.concatenate([[0], u_int, [0]])
        res = []
        for i in range(1, n_points-1):
            u_xx = (u_full[i+1] - 2*u_full[i] + u_full[i-1]) / h**2
            res.append(u_xx + u_full[i]**2 - np.sin(np.pi * x[i]))
        return res
    u_int_guess = np.zeros(n_points-2)
    u_int = fsolve(func, u_int_guess, xtol=1e-8)
    return x, np.concatenate([[0], u_int, [0]])

# Generate approximate true solution
x_true, u_true = solve_pde_fd(100)
interp_u_true = interp1d(x_true, u_true, kind='cubic')

# Generate data points with true values
def generate_data(n_points):
    x_data = torch.linspace(0, 1, n_points, device=device).unsqueeze(1)
    u_data = torch.tensor(interp_u_true(x_data.cpu().numpy()), dtype=torch.float32, device=device).unsqueeze(1)
    return x_data, u_data

# Define the PINN model
class PINN(nn.Module):
    def __init__(self, layers):
        super(PINN, self).__init__()
        self.activation = nn.Tanh()
        self.layers = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])
    
    def forward(self, x):
        out = x
        for layer in self.layers[:-1]:
            out = self.activation(layer(out))
        out = self.layers[-1](out)
        return out

# Compute PDE residual: u''(x) + u(x)^2 - sin(pi*x) = 0
def pde_residual(model, x):
    x = x.clone().detach().requires_grad_(True)
    u = model(x)
    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]
    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]
    res = u_xx + u**2 - torch.sin(torch.pi * x)
    return res

# Training function with debugging
def train_PINN(lambda_f, num_epochs=3000, print_every=1500):
    model = PINN(layers=[1, 30, 30, 1]).to(device)
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    
    # Verify model parameters
    for name, param in model.named_parameters():
        if not param.requires_grad:
            raise ValueError(f"Parameter {name} does not require grad!")
    
    # Data points
    n_data = 40
    x_data, u_data = generate_data(n_data)
    x_data_train, u_data_train = x_data[:n_data//2], u_data[:n_data//2]
    x_data_test, u_data_test = x_data[n_data//2:], u_data[n_data//2:]
    
    # Collocation points
    n_r = 20
    x_r = torch.linspace(0, 1, n_r, device=device).unsqueeze(1)
    n_r_test = 50
    x_r_test = torch.linspace(0, 1, n_r_test, device=device).unsqueeze(1)
    
    mse_loss = nn.MSELoss()
    
    # Training loop
    for epoch in range(num_epochs):
        optimizer.zero_grad()
        
        # Data loss
        u_pred_train = model(x_data_train)
        L_u = mse_loss(u_pred_train, u_data_train)
        
        # Physics loss
        res_train = pde_residual(model, x_r)
        L_f = mse_loss(res_train, torch.zeros_like(res_train))
        
        # Total loss
        loss = L_u + lambda_f * L_f
        
        # Debugging prints
        if epoch % print_every == 0:
            print(f"Epoch {epoch}:")
            print(f"  L_u: {L_u.item():.4e}, grad_fn: {L_u.grad_fn}")
            print(f"  L_f: {L_f.item():.4e}, grad_fn: {L_f.grad_fn}")
            print(f"  Loss: {loss.item():.4e}, grad_fn: {loss.grad_fn}")
        
        loss.backward()
        optimizer.step()
    
    # Evaluation 
    u_pred_train = model(x_data_train)
    L_u_train = mse_loss(u_pred_train, u_data_train).item()
    u_pred_test = model(x_data_test)
    L_u_test = mse_loss(u_pred_test, u_data_test).item()
    
    # For physics loss evaluation, we need gradients
    res_train_eval = pde_residual(model, x_r)
    L_f_train = mse_loss(res_train_eval, torch.zeros_like(res_train_eval)).item()
    res_test_eval = pde_residual(model, x_r_test)
    L_f_test = mse_loss(res_test_eval, torch.zeros_like(res_test_eval)).item()
    
    return L_u_train, L_f_train, L_u_test, L_f_test

# Runing experiments
lambda_f_values = [0.0, 1.0, 5.0, 10.0, 20.0, 50.0]
O_ratios, P_ratios, R_bounds = [], [], []

n_r = 20  # Number of training collocation points
C1 = 1.0

for lam in lambda_f_values:
    print(f"\nTraining with lambda_f = {lam}")
    L_u_train, L_f_train, L_u_test, L_f_test = train_PINN(lam)
    
    O = L_u_test / L_u_train if L_u_train > 0 else np.nan
    P = L_f_test / L_f_train if L_f_train > 0 else np.nan
    R_bound = L_u_train + C1 * np.log(n_r) / np.sqrt(n_r + lam)
    
    O_ratios.append(O)
    P_ratios.append(P)
    R_bounds.append(R_bound)
    
    print(f"O = {O:.4e}, P = {P:.4e}, R_bound = {R_bound:.4e}")

# Plotting
plt.figure(figsize=(14, 4))

plt.subplot(1, 3, 1)
plt.plot(lambda_f_values, O_ratios, 'ro-', label=r'$\mathcal{O}$')
plt.xlabel(r'$\lambda_f$')
plt.ylabel(r'$\mathcal{O}$ (Data Overfitting Ratio)')
plt.title('Data Overfitting Ratio vs. $\lambda_f$')
plt.legend()
plt.grid(True)

plt.subplot(1, 3, 2)
plt.plot(lambda_f_values, P_ratios, 'bo-', label=r'$\mathcal{P}$')
plt.xlabel(r'$\lambda_f$')
plt.ylabel(r'$\mathcal{P}$ (Physics Enforcement Ratio)')
plt.title('Physics Enforcement Ratio vs. $\lambda_f$')
plt.legend()
plt.grid(True)

plt.subplot(1, 3, 3)
plt.plot(lambda_f_values, R_bounds, 'go-', label='Generalization Bound')
plt.xlabel(r'$\lambda_f$')
plt.ylabel(r'$R_{\rm bound}$')
plt.title('Generalization Bound vs. $\lambda_f$')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Plotting the learned solution vs. true solution
model = PINN(layers=[1, 30, 30, 1]).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
x_data, u_data = generate_data(40)
x_r = torch.linspace(0, 1, 20, device=device).unsqueeze(1)
mse_loss = nn.MSELoss()
for epoch in range(3000):
    optimizer.zero_grad()
    L_u = mse_loss(model(x_data), u_data)
    L_f = mse_loss(pde_residual(model, x_r), torch.zeros_like(pde_residual(model, x_r)))
    loss = L_u + lambda_f_values[-1] * L_f
    loss.backward()
    optimizer.step()

x_plot = torch.linspace(0, 1, 100, device=device).unsqueeze(1)
with torch.no_grad():
    u_pred = model(x_plot).cpu().numpy()
plt.figure()
plt.plot(x_true, u_true, 'b-', label='True Solution')
plt.plot(x_plot.cpu().numpy(), u_pred, 'r--', label='PINN Prediction')
plt.xlabel('x')
plt.ylabel('u(x)')
plt.legend()
plt.title('True vs. Predicted Solution')
plt.grid(True)
plt.show()